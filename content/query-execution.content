<!--title: Query Execution-->
<section id="queryexec">
<h2>OpenTSDB Query Execution</h2>

This page sheds some light on how OpenTSDB retrieves and processes data when
it serves a query.  It is useful to understand what the results mean and how
to craft queries that yield something meaningful.

<h3>Life of a Query: Overview</h3>

All queries have:
<ul>
<li>A metric name for which to retrieve data;</li>
<li>A start time;</li>
<li>A stop time (optional, if not set, assumed to be "now");</li>
<li>A possibly empty set of tags to filter the data
(e.g. <code>host=foo</code>, or wildcards such as <code>host=*</code>);</li>
<li>An aggregation function (e.g. <code>sum</code> or <code>avg</code>);</li>
<li>Whether or not to get the "rate of change" the data (in mathematical
terms: the first derivative).</li>
<li>Optionally: a downsampling interval (e.g. 10 minutes) and downsampling
function (e.g. <code>avg</code>)</li>
</ul>

Here's an overview of the steps taken to serve a query:
<ol>
<li>Open a scanner, set with a start key composed of the metric requested and
the start time.</li>
<li>The scanner is configured to stop at a key corresponding to the stop time
and to filter out rows containing data with tags that don't match the tags
we're looking for.</li>
<li>If any multiple choice tags (e.g. <code>host=foo|bar</code>) or wildcard
tags are used (e.g. <code>host=*</code>, which is akin to a <code>GROUP
BY</code> in SQL), sort the rows in groups accordingly.<br/>
For each group, repeated the remaining steps:</li>
<li>Apply the downsample function, if there is one.  For instance
<code>10m-avg</code> will collapse each consecutive chunk of 600 second worth
of data down to one data point using the average.</li>
<li>Aggregate the values of the different time series together (for instance
<code>sum</code> will sum up all the time series that wound up being together
in this group &ldash; this requires that you understand how to perform such
operations on time series in a sound fashion, see below).</li>
<li>If the rate of change was requested, compute that using the previous value
returned.</li>
</ol>

The entire process has O(n) complexity, "n" being the number of data points,
and in particular the code tries hard to make only a single-pass.

<h3 id="aggregation">Time Series Aggregation</h3>
This step deserves a bit more explanations.  Given two (or more) time series,
what is the meaning of "the sum of the time series"?  Or their average?  Or
the max or min?

<p>
The max seems fairly intuitive: at each time <i>T</i> of each data point, look
at the value that all other time series have, and pick whichever one has the
highest value.  The sum (and other functions) works in a similar way: at each
time <i>T</i> of each data point of the time series we're trying to sum up,
take the values of all the time series at time <i>T</i>, and sum their values.
This is fairly intuitive again, it's akin to "stacking up" each line of the
graph.

<p>
However not all time series may have a data point at time <i>T</i>, because
they don't have to progress in lockstep.  This is why
<strong>interpolation</strong> is needed.  When a time series doesn't have a
data point at time <i>T</i>, we need to find a sensible value for this time
series at time <i>T</i> that can be aggregated with other time series.  The
following graph illustrates this quite well.

<p>
The "sum of the m" is the blue line at the top.  It's made of the sum
of the red line (for <code>host=foo</code>) and the green line (for
<code>host=bar</code>):<br/>
<img src="img/with-lerp.png" alt="Aggregation by sum with interpolation" />
<br/>
It seems intuitive from the image above that if you "stack up" the red line
and the green line, you'd get the blue line.  At any discrete point in time,
the blue line has a value that is equal to the sum of the value of the red
line at that time, and the value of the green line at that time.  Without
interpolation, you get something rather unintuitive that is harder to make
sense of, and which is also a lot less meaningful and useful:<br/>
<img src="img/without-lerp.png" alt="Aggregation by sum with interpolation" />
<br/>
No need to be a mathematician or to have taken advanced maths classes to see
that interpolation is needed to properly aggregate multiple time series
together and get meaningful results.

<p>
Note that at the moment OpenTSDB only supports
<a href="http://en.wikipedia.org/wiki/Linear_interpolation">linear interpolation</a>
(sometimes shortened "lerp") for sake of simplicity.  Patches are welcome for
those who would like to add other interpolation methods.

<p>
Here is another slightly more complicated example that came from the mailing
list, depicting how multiple time series are aggregated by average:<br/>
<img src="img/aggregation-average.png" alt="Aggregation by average with interpolation" />
<br/>
As we can see, the resulting time series has one data point at each timestamp
of all the underlying time series it aggregates, and that data point is
computed by taking the average of the values of all the time series at that
timestamp.  This is also true for the lonely data point of the squared-purple
time series, that temporarily boosted the average until the next data point.

</section>
